from typing_extensions import TypedDict

from pydantic import BaseModel, Field
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables.config import RunnableConfig


class AgentState(TypedDict):
    current_chunk_index: int
    chunks_summaries: list[str]
    is_sufficient: bool
    generated_ideas: list[str]


class _ChunkSummaryOutput(BaseModel):
    summary: str = Field(
        ...,
        description="Summary of the chunk generated by the LLM.",
    )
    is_sufficient: bool = Field(
        ...,
        description="Indicates if the summary is sufficient for generating image ideas.",
    )


class _GeneratedIdeas(BaseModel):
    ideas: list[str] = Field(
        ...,
        description="List of image ideas generated from the chunk summaries.",
    )


def summarize_chunk_node(state: AgentState, config: RunnableConfig):
    # Extract the prompt parameters and LLM
    chunk_idx = state["current_chunk_index"]
    chunks_summaries = state["chunks_summaries"]
    chunk_text = config["configurable"]["chunks"][chunk_idx]
    prompt_str = config["configurable"]["summarize_chunk_prompt"]
    max_summary_size = config["configurable"]["max_summary_size"]
    llm = config["configurable"]["llm_model"]

    # Invoke the LLM to summarize the chunk
    structured_llm = llm.with_structured_output(_ChunkSummaryOutput)
    prompt = ChatPromptTemplate.from_template(prompt_str)
    chain = prompt | structured_llm
    summary: _ChunkSummaryOutput = chain.invoke(
        {
            "chunk_text": chunk_text,
            "max_summary_size": max_summary_size,
            "chunks_summaries": chunks_summaries,
        }
    )

    # Update the state with the chunk summary
    state["current_chunk_index"] += 1
    state["chunks_summaries"].append(summary.summary)
    state["is_sufficient"] = summary.is_sufficient

    print("[Agent] Generated chunk summary:")
    print(f"\t[-] Chunk index: {chunk_idx}")
    print(f"\t[-] Chunk summary: {summary.summary}")
    print(f"\t[-] Is sufficient: {summary.is_sufficient}")

    return state


def generate_image_ideas_node(state: AgentState, config: RunnableConfig):
    # Extract the prompt parameters
    chunks_summaries = state["chunks_summaries"]
    total_ideas = config["configurable"]["total_ideas"]
    prompt_str = config["configurable"]["generate_image_ideas"]
    llm = config["configurable"]["llm_model"]

    # Invoke the LLM to generate image ideas
    structured_llm = llm.with_structured_output(_GeneratedIdeas)
    prompt = ChatPromptTemplate.from_template(prompt_str)
    chain = prompt | structured_llm
    generated_ideas: _GeneratedIdeas = chain.invoke(
        {
            "chunks_summaries": chunks_summaries,
            "total_ideas": total_ideas,
        }
    )

    # Update the state with the generated image ideas
    state["generated_ideas"] = generated_ideas.ideas

    print("[Agent] Generated image ideas:")
    for idx, idea in enumerate(generated_ideas.ideas):
        print(f"\t[-] Idea {idx + 1}: {idea}")

    return state


def continue_summarizing_conditional_edge(
    state: AgentState, config: RunnableConfig
) -> str:
    """
    Conditional edge to determine if the agent should continue summarizing chunks.
    If there are more chunks to summarize and the summary is not sufficient,
    it returns the next node to summarize the chunk. Otherwise, it returns the
    node to generate image ideas.

    Args:
        state (AgentState): The current state of the agent.
        config (RunnableConfig): The configuration for the agent.

    Returns:
        str: The next node to execute.
    """
    total_chunks = config["configurable"]["total_chunks"]
    is_sufficient = state["is_sufficient"]
    chunk_idx = state["current_chunk_index"]

    if chunk_idx < total_chunks and not is_sufficient:
        return "summarize_chunk"

    return "generate_image_ideas"
