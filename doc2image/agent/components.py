from typing_extensions import TypedDict

from pydantic import BaseModel, Field
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables.config import RunnableConfig


class AgentState(TypedDict):
    current_chunk_index: int
    chunks_summaries: list[str]
    is_sufficient: bool
    global_summary: str
    image_prompts: list[str]


class _ChunkSummaryOutput(BaseModel):
    summary: str = Field(
        ...,
        description="Summary of the chunk generated by the LLM.",
    )
    is_sufficient: bool = Field(
        ...,
        description="Indicates if the summary is sufficient for generating image ideas.",
    )


class _ImagePrompts(BaseModel):
    prompts: list[str] = Field(
        ...,
        description="List of image prompts based on the document summary.",
    )


def summarize_chunk_node(state: AgentState, config: RunnableConfig):
    # Extract the prompt parameters and LLM
    chunk_idx = state["current_chunk_index"]
    chunks_summaries = state["chunks_summaries"]
    chunk_text = config["configurable"]["chunks"][chunk_idx]
    prompt_str = config["configurable"]["summarize_chunk_prompt"]
    max_chunk_summary_size = config["configurable"]["max_chunk_summary_size"]
    llm = config["configurable"]["llm_model"]

    # Invoke the LLM to summarize the chunk
    structured_llm = llm.with_structured_output(_ChunkSummaryOutput)
    prompt = ChatPromptTemplate.from_template(prompt_str)
    chain = prompt | structured_llm
    summary: _ChunkSummaryOutput = chain.invoke(
        {
            "chunk_text": chunk_text,
            "max_chunk_summary_size": max_chunk_summary_size,
            "chunks_summaries": chunks_summaries,
        }
    )

    # Update the state with the chunk summary
    state["current_chunk_index"] += 1
    state["chunks_summaries"].append(summary.summary)
    state["is_sufficient"] = summary.is_sufficient

    print("[Agent] Generated chunk summary:")
    print(f"\t[-] Chunk index: {chunk_idx}")
    print(f"\t[-] Chunk summary: {summary.summary}")
    print(f"\t[-] Is sufficient: {summary.is_sufficient}")

    return state


def generate_global_summary_node(state: AgentState, config: RunnableConfig):
    # Extract the prompt parameters
    chunks_summaries = state["chunks_summaries"]
    max_global_summary_size = config["configurable"]["max_global_summary_size"]
    prompt_str = config["configurable"]["generate_global_summary_prompt"]
    llm = config["configurable"]["llm_model"]

    # Invoke the LLM to generate the global summary
    prompt = ChatPromptTemplate.from_template(prompt_str)
    chain = prompt | llm | StrOutputParser()
    global_summary: str = chain.invoke(
        {
            "chunks_summaries": chunks_summaries,
            "max_global_summary_size": max_global_summary_size,
        }
    )

    # Update the state with the global summary
    state["global_summary"] = global_summary

    print(f"[Agent] Generated global summary: {global_summary}")

    return state


def generate_image_prompts_node(state: AgentState, config: RunnableConfig):
    # Extract the prompt parameters
    global_summary = state["global_summary"]
    total_prompts_to_generate = config["configurable"]["total_prompts_to_generate"]
    prompt_str = config["configurable"]["generate_image_prompts_prompt"]
    llm = config["configurable"]["llm_model"]

    # Invoke the LLM to generate image ideas
    structured_llm = llm.with_structured_output(_ImagePrompts)
    prompt = ChatPromptTemplate.from_template(prompt_str)
    chain = prompt | structured_llm
    image_prompts: _ImagePrompts = chain.invoke(
        {
            "global_summary": global_summary,
            "total_prompts_to_generate": total_prompts_to_generate,
        }
    )

    # Update the state with the generated image ideas
    state["image_prompts"] = image_prompts.prompts

    print("[Agent] Generated image prompts:")
    for idx, idea in enumerate(image_prompts.prompts):
        print(f"\t[-] Prompts {idx + 1}: {idea}")

    return state


def continue_summarizing_conditional_edge(
    state: AgentState, config: RunnableConfig
) -> str:
    """
    Conditional edge to determine if the agent should continue summarizing chunks.
    If there are more chunks to summarize and the summary is not sufficient,
    it returns the next node to summarize the chunk. Otherwise, it returns the
    node to generate image ideas.

    Args:
        state (AgentState): The current state of the agent.
        config (RunnableConfig): The configuration for the agent.

    Returns:
        str: The next node to execute.
    """
    total_chunks = config["configurable"]["total_chunks"]
    is_sufficient = state["is_sufficient"]
    chunk_idx = state["current_chunk_index"]

    if chunk_idx < total_chunks and not is_sufficient:
        return "summarize_chunk"

    return "generate_global_summary"
